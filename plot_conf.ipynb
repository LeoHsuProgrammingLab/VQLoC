{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vision/hwjiang/anaconda3/envs/vq2d/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pprint\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import itertools\n",
    "import argparse\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "from config.config import config, update_config\n",
    "from utils import exp_utils\n",
    "from evaluation import eval_utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks, medfilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'val'\n",
    "annotation_path = os.path.join('/vision/hwjiang/episodic-memory/VQ2D/data', 'vq_{}.json'.format(mode))\n",
    "with open(annotation_path) as fp:\n",
    "    annotations = json.load(fp)\n",
    "clipwise_annotations_list = eval_utils.convert_annotations_to_clipwise_list(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path ='/vision/hwjiang/vq2d/output/ego4d_vq2d/vq2d_all_transformer2_anchor_dinov2_inference/default-arch_aug-clip_anchor_focal-w10_2heads_dropout0.2_positive0.3_inference_noqs/inference_cache_val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_scores(pred, gt, annot_key, name):\n",
    "    '''\n",
    "    both in shape [N]\n",
    "    '''\n",
    "    assert name in ['raw', 'smooth', 'smooth_thresh']\n",
    "    save_path = './visualization/plot_scores'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    N = pred.shape[0]\n",
    "    x = np.arange(N)\n",
    "\n",
    "    pred_sm = medfilt(pred, kernel_size=5)\n",
    "\n",
    "    if name == 'raw':\n",
    "        plt.plot(x, pred, marker=None, color='b', label='pred')\n",
    "    elif name == 'smooth':\n",
    "        plt.plot(x, pred_sm, marker=None, color='g', label='pred_sm')\n",
    "    else:\n",
    "        pred_sm[pred_sm < 0.5] = 0\n",
    "        plt.plot(x, pred_sm, marker=None, color='g', label='pred_sm_thre')\n",
    "    plt.plot(x, gt, marker=None, color='r', label='gt')\n",
    "    plt.xlabel('number of frames')\n",
    "    plt.ylabel('occurance score')\n",
    "    plt.ylim((0.0, 1.05))\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    plt.savefig(os.path.join(save_path, annot_key + f'_{name}.jpg'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 132 is out of bounds for dimension 0 with size 132",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mfor\u001b[39;00m it \u001b[39min\u001b[39;00m annot[\u001b[39m'\u001b[39m\u001b[39mresponse_track\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     25\u001b[0m     gt_idx\u001b[39m.\u001b[39mappend(it[\u001b[39m'\u001b[39m\u001b[39mframe_number\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 26\u001b[0m gt_scores[gt_idx] \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n\u001b[1;32m     28\u001b[0m draw_scores(ret_scores\u001b[39m.\u001b[39mnumpy(), gt_scores\u001b[39m.\u001b[39mnumpy(), annot_key, \u001b[39m'\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m draw_scores(ret_scores\u001b[39m.\u001b[39mnumpy(), gt_scores\u001b[39m.\u001b[39mnumpy(), annot_key, \u001b[39m'\u001b[39m\u001b[39msmooth\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 132 is out of bounds for dimension 0 with size 132"
     ]
    }
   ],
   "source": [
    "i = 20\n",
    "for _, annots in clipwise_annotations_list.items():\n",
    "    clip_uid = annots[0][\"clip_uid\"]\n",
    "    clip_dir = '/vision/srama/Research/Ego4D/episodic-memory/VQ2D/data/clips_fullres'\n",
    "    clip_path = os.path.join(clip_dir, clip_uid  + '.mp4')\n",
    "    keys = [\n",
    "            (annot[\"metadata\"][\"annotation_uid\"], annot[\"metadata\"][\"query_set\"])\n",
    "            for annot in annots\n",
    "        ]\n",
    "    for key, annot in zip(keys, annots):\n",
    "\n",
    "        annotation_uid = annot[\"metadata\"][\"annotation_uid\"]\n",
    "        query_set = annot[\"metadata\"][\"query_set\"]\n",
    "        annot_key = f\"{annotation_uid}_{query_set}\"\n",
    "        query_frame = annot[\"query_frame\"]\n",
    "        visual_crop = annot[\"visual_crop\"]\n",
    "        \n",
    "        save_path = os.path.join(cache_path, f'{annot_key}.pt')\n",
    "        cache = torch.load(save_path)\n",
    "        ret_bboxes, ret_scores = cache['ret_bboxes'], torch.sigmoid(cache['ret_scores'])\n",
    "\n",
    "        gt_scores = torch.zeros_like(ret_scores)\n",
    "        gt_idx = []\n",
    "        for it in annot['response_track']:\n",
    "            gt_idx.append(it['frame_number'])\n",
    "        gt_scores[gt_idx] = 1.0\n",
    "\n",
    "        draw_scores(ret_scores.numpy(), gt_scores.numpy(), annot_key, 'raw')\n",
    "        draw_scores(ret_scores.numpy(), gt_scores.numpy(), annot_key, 'smooth')\n",
    "        draw_scores(ret_scores.numpy(), gt_scores.numpy(), annot_key, 'smooth_thresh')\n",
    "        i -= 1\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annot['response_track'][0]['frame_number']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vq2d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
